{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DCQ-Continuous algorithm:\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extrapolation error: unseen state-action pairs are erroneously estimated to have unrealistic values. Inability to learn truly off-policy. (Main reason why we cannot use normal off-policy algorithms straightaway in an offline problem.)\n",
    "\n",
    "Extrapolation error can be attributed to a mismatch in the distribution of data induced by the policy and the\n",
    "distribution of data contained in the batch. As a result, it may be impossible to learn a value function for a\n",
    "policy which selects actions not contained in the batch.\n",
    "\n",
    "ToDo: Example of off-policy RL on data -> It should failed!\n",
    "\n",
    "ToDo: Add plot of more realistic cases (See paper BCQ fig-1)\n",
    "\n",
    "ToDo: Add first offline RL attempts: Dagger (Imitation Learning) (2010) - Deep Q-learning for demostrations(2017)\n",
    "\n",
    "BCQ (2019): agents are trained to maximize reward while minimizing the mismatch between the state-action visitation of the\n",
    "policy and the state-action pairs contained in the batch (**Note**: This can be done nowadays in different ways and\n",
    "BCQ is a very particular one - See Levine review for a more general description.)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In formulas:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "$\\[\n",
    "\\Tau^{\\pi} Q(s, a) \\approx \\mathbb{E}_{s' \\sim B}[r + \\gamma Q(s', \\pi(s'))]\n",
    "\\]$\n",
    "\n",
    "The problem is that if $a' = \\pi(s')$ doesn't belong to the dataset $\\sim B$ we will have what is known as an 'extrapolation error' as we will produce out of distribution data.\n",
    "\n",
    "IMPORTANT!!!: During training only distribution shift in actions but in inference could be distribution\n",
    "shift in states.\n",
    "\n",
    "This could be bad because there could be a reason why this data is not in the dataset (ToDo: give a realistic example) .\n",
    "\n",
    "\n",
    "Show these cases with examples-->se Q values for different examples to see\n",
    "what is going on!!\n",
    "\n",
    "\n",
    "ToDo: In my example of the 2D grid env this is not nice as the o.o.d values reaches the target and I should find an example where o.o.d. gives a wrong result. Maybe I could say that the expert collector (behavior policy) in my example realizes that close to the target there is an object and so it stops before. The task should be to reach the best trajectory within the data provided.\n",
    "\n",
    "Question: what if you put a reward zero around the o.o.d data? Not a solution as many undesired effects: Q values convergence difficult (you jump from a nice reward to another bad locally $s$ and $s'$ neighboors for instance). In continuous cases it could be that is difficult for the policy to distinguish between $s$ and $s'$ . A very low reward could limit exploration (I could add a zone around the target).\n",
    "\n",
    "ToDo: BCQ continuous paper: Theory of extrapolation errors.\n",
    "\n",
    "Imitation Learning: No notion of reward. The logits are not related with Q_value sin general.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CQL: 2020 (Levine)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ToDo: 8x8_v1 dataset --> dqn --> see ood and Q vs R (simple data) (data with repulsive size) (in both cases we should have ood) -- Compare wit bcq (high phi Q~R_t and not too many ood data but not for big phi)\n",
    "\n",
    "ToDo: similar analysis but with cql\n",
    "\n",
    "ToDo: ICQL and AWR\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Questions / Comments:\n",
    "0 - Read Annes part\n",
    "1 - Partially observable MDP - Model base RL\n",
    "2 - Why the data can be sampled from: d_pi_beta(s)*pi_beta(a/s) with d_pi_beta(s) the discount state distribution. Why the discount?\n",
    "3 - Give pros and cons of different offline algorithms\n",
    "4 - refresh mind about different famous RL algorithms\n",
    "5 - Why there is overstimation of Q(s,a) in offline RL\n",
    "\n",
    "PROBLEM --> With CQL continuous I should get the q-values from the critic but not working somehow....\n",
    "\n",
    "\n",
    "Exercises:\n",
    "\n",
    "1 - Imitation learning (look at Q_values + oof polts, etc.) (supervise learning)\n",
    "\n",
    "a - semioptimal behavior policy.\n",
    "\n",
    "\n",
    "2 - Train off policy methods (dqn) --> Q_values vs Values mismatch -- ood data.\n",
    "\n",
    "3 - General overview of different offline RL approaches (Levine).\n",
    "\n",
    "4 - Offline RL from data - Dagger, etc (first cases)\n",
    "\n",
    "4 - Introduce BCQ and train it!\n",
    "a - Play with parameter:\n",
    "    a1 - imitation\n",
    "    a2 - dqn\n",
    "    a3 - middle --> achieve task without going ood\n",
    "\n",
    "5 - BCQ vs DQN in complex gym environment --> BCQ paper results.\n",
    "\n",
    "5 - CQL\n",
    "\n",
    "6 - AWS\n",
    "\n",
    "7 - IQL\n",
    "\n",
    "8 - DIQL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Theory:\n",
    "\n",
    "What is offline RL\n",
    "\n",
    "Why is it difficult?\n",
    "\n",
    "1 - Not possible to improve exploration: As the learning algorithm must rely entirely on the static\n",
    "dataset D, there is no possibility of improving exploration: exploration is outside the scope of the\n",
    "algorithm, so if D does not contain transitions that illustrate high-reward regions or covers a wide range of tasks in the state space, it may be impossible to discover those regions (this is opposite ti online RL where you explore by interacting with the environment). As there is nothing that we can do to address this challenge, we will not spend any more time on it, and will instead assume that D adequately covers the space we are interested in to make learning feasible. (If some states are not in the dataset could be for a good reason, maybe there is an obstacle for the robot there or a forbiden....)\n",
    "\n",
    "\n",
    "2 - Distributional shift: To improve the learned policy beyond what's in the dataset D, we need to take actions that differ from those in the dataset. This challenges many existing machine learning methods, which assume that data is independent and identically distributed (i.i.d.). In standard supervised learning, we aim to train a model that performs well on data from the same distribution as the training data. In offline RL, our goal is to learn a policy that behaves differently (hopefully better) than what's seen in the dataset D.\n",
    "\n",
    "\n",
    "IMPORTANT: During evaluation we could came up with states not included in the dataset (i.e. training)!!! This happens in my exercise in the 2D grid square zone not explore by the behavior policy.\n",
    "\n",
    "\n",
    "Do a simple plot to show this here!!!!\n",
    "\n",
    "\n",
    "Add theorems 2.1-2.2 (Levine review) here that give a look at ood errors in Behavioral clonning.\n",
    "\n",
    "\n",
    "Can we use standard online methods to offline RL?\n",
    "\n",
    "\n",
    "A - IMPORTANCE SAMPLING METHODS AND POLICY GRADIENTS:\n",
    "\n",
    "Q: There are not ood issues with these methods??\n",
    "\n",
    "1 - Importance sampling methods: As we already have a behavior policy (or we could construct it from data like\n",
    "behavioral clonning) we could apply policy gradient methods to eq.5 (Levine review) but too many cons with this approach. The maximum improvement that can be reliably obtained via importance sampling is limited by:\n",
    " (i) the suboptimality of the behavior policy;\n",
    " (ii) the dimensionality of the state and action space;\n",
    " (iii) the effective horizon of the task\n",
    "\n",
    "\n",
    "**Note** **(Not include it in the workshop)**: The second challenge is that the most effective of these off-policy policy gradient methods either requires estimating the value function, or the state-marginal density ratios via dynamic programming. As\n",
    "several prior works have shown, and as we will review in Section 4, dynamic programming methods\n",
    "suffer from issues pertaining to out-of-distribution queries in completely offline settings, making it\n",
    "hard to stably learn the value function without additional corrections\n",
    "\n",
    "\n",
    "IMPORTANT!!! : I should check what happens if in testing I start in a non-seen state during training.\n",
    "In theory I should scape quickly from there???\n",
    "\n",
    "\n",
    "B - OFFLINE RL VIA DYNAMIC PROGRAMMING (section 4 - Levine)\n",
    "\n",
    "\n",
    "    Out of distribution data or distribution shift: Solutions to this issue can be broadly grouped into two categories:\n",
    "\n",
    "    a - policy constraint methods, discussed in Section 4.3, which constrain the learned\n",
    "        policy π to lie close to the behavior policy πβ, thus mitigating distributional shift, (AWR - IQL)\n",
    "\n",
    "    b - Constrain at the action level (BCQ)\n",
    "\n",
    "    c - uncertaintybased methods, discussed in Section 4.4, which attempt to estimate the epistemic uncertainty of\n",
    "        Q-values, and then utilize this uncertainty to detect distributional shift (CQL)\n",
    "\n",
    "\n",
    "    (To add or not to add: Linear Value Functions: Only for historical reason as nowadays the value functions are estimate with DNN, i.e they are non-linear. Anyway both linear and non-linear dynamic programming methods are always affected by distributional shifts)\n",
    "\n",
    "    a -\n",
    "\n",
    "\n",
    "\n",
    "Important comment: In standard online reinforcement learning, such issues (overestimation of Q_value due to action out of distribution) are corrected naturally when the policy acts in the environment, attempting the transitions it (erroneously) believes to be good, and observing that in fact they are not. However, in the offline setting, the policy cannot correct such over-optimistic Q-values, and these errors accumulate over each iteration of training, resulting in arbitrarily poor\n",
    "final results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q: How safe is offline RL, i.e. how accurate is to control if we are out of distribution. In online we have a simulation\n",
    "to deal with!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1 - Intro to Offline RL:\n",
    "\n",
    "    To be added: Inverse RL\n",
    "\n",
    "    a - potential applications\n",
    "    b - Imitation learning, BC algorithm\n",
    "    c - Dagger\n",
    "\n",
    "    d - Create a simple behavior policy (maybe random and sub-optimal)\n",
    "       d.1 - Create a sub-optimal policy in a Mujoco environments -> maybe a not fully trained?\n",
    "             - See how they are compute in D4RL\n",
    "    e - Collect data (show minari wrapper)\n",
    "    f - Add collected data to tianshou reply buffer\n",
    "\n",
    "    Examples with Grid and Mujoco\n",
    "\n",
    "2 - Issues from a Math point of view\n",
    "\n",
    "    a - OOD: Example of an offline RL (SAC - DQN)\n",
    "        a.1 - show how to train a model offline in Tianshou.\n",
    "        a.2 - OOD in state in inference !! (add example) --> how to tackle this??\n",
    "    b - Show offline data distribution (Exercise? - Visualization)\n",
    "        b.1 - Why OOD is bad?\n",
    "    c - Different examples and behavior policies?\n",
    "\n",
    "3 - Solutions:\n",
    "\n",
    "    a - Show different kind of approaches (algorithm families)\n",
    "    b - Review the most important algorithms\n",
    "    c - Apply algorithms to simple Grid environments and show balance imitation vs ood .\n",
    "    d -\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
