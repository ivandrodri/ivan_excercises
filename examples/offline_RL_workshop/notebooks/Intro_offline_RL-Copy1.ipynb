{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ivan/anaconda3/envs/tianshou_dev/lib/python3.8/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment GymV26Environment-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/home/ivan/anaconda3/envs/tianshou_dev/lib/python3.8/site-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  pkg_resources.declare_namespace(__name__)\n",
      "/home/ivan/anaconda3/envs/tianshou_dev/lib/python3.8/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(parent)\n",
      "/home/ivan/anaconda3/envs/tianshou_dev/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/home/ivan/anaconda3/envs/tianshou_dev/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n",
      "/home/ivan/.local/lib/python3.8/site-packages/redis/connection.py:67: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  hiredis_version = StrictVersion(hiredis.__version__)\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    }
   ],
   "source": [
    "from examples.offline_RL_workshop.offline_policies.policy_registry import PolicyType\n",
    "from examples.offline_RL_workshop.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "from examples.offline_RL_workshop.custom_envs.custom_envs_registration import register_grid_envs\n",
    "import gymnasium as gym\n",
    "from examples.offline.utils import load_buffer_minari\n",
    "from examples.offline_RL_workshop.visualizations.utils import get_state_action_data_and_policy_grid_distributions\n",
    "from examples.offline_RL_workshop.utils import state_action_histogram, compare_state_action_histograms\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from examples.offline_RL_workshop.custom_envs.custom_envs_registration import RenderMode, CustomEnv\n",
    "from examples.offline_RL_workshop.behavior_policies.visualize_custom_policies import render_custom_policy_simple_grid\n",
    "from examples.offline_RL_workshop.behavior_policies.behavior_policy_registry import BehaviorPolicyType\n",
    "\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/home/ivan/.mujoco/mujoco210/bin\"\n",
    "os.environ[\"MINARI_DATASETS_PATH\"] = \"/home/ivan/Documents/GIT_PROJECTS/Tianshou/tianshou/offline_data\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Introduction:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problem definition: ToDo\n",
    "Examples of applicability:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In summary, the process is simple: \n",
    "\n",
    "Phase A - Collect data set, $D$, of state-action paird through a behavior policy (also known as expert policy) $\\pi_b$: e.g. a robot randomly moving (or human controlled) in a given space, data collected from an autonomous vehicle, etc. The data collected doesn't need to come from an expert (typically the case in real situations) and during this phase we are not worry in general about a specific task (i.e. rewards). In fact it could be that the data is collected from a robot doing a different task that the one we are interested in. We want just a set of allowed state-action pairs that could be usable for the task in mind.\n",
    "\n",
    "Phase B - In this phase we want to solve a given task (so we need to design rewards) only through the provided initial data without any interaction with the environment but still be able to find an optimal or near-optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see phase A and B with a simple [example](https://docs.google.com/presentation/d/1-cfO7MNcH6iyN4EwyFjI9F3cVu4hYn5ihCqmZHBeeSk/edit#slide=id.g28654ea4ec6_0_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase A:\n",
    "\n",
    "1 - Generate MINARI Dataset\n",
    "2 - CREATE BEHAVIOR POLICY\n",
    "3 - LOOK AT DATA DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T13:35:08.419547Z",
     "start_time": "2023-10-04T13:35:08.408763Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m register_grid_envs()\n\u001b[1;32m     10\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(ENV_NAME, render_mode\u001b[38;5;241m=\u001b[39mRENDER_MODE)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mrender_custom_policy_simple_grid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENV_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRENDER_MODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbehavior_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBEHAVIOR_POLICY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#Load data\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#data = load_buffer_minari(DATA_SET_NAME)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print(f\"number of elements: {len(data)}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#state_action_histogram(state_action_count_policy)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#compare_state_action_histograms(state_action_count_data, state_action_count_policy)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GIT_PROJECTS/Tianshou/tianshou/examples/offline_RL_workshop/behavior_policies/visualize_custom_policies.py:27\u001b[0m, in \u001b[0;36mrender_custom_policy_simple_grid\u001b[0;34m(env_name, render_mode, behavior_policy, num_steps)\u001b[0m\n\u001b[1;32m     25\u001b[0m     action \u001b[38;5;241m=\u001b[39m behavior_policy(env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mbehavior_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m next_state, reward, done, time_out, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     30\u001b[0m num_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_5x5_discrete_V1 #\"Grid_2D_5x5_discrete-V1\"\n",
    "DATA_SET_NAME = \"Grid_2D_5x5_discrete-V1_data\" #ENV_NAME + \"_data\"\n",
    "NUM_STEPS = 1000\n",
    "RENDER_MODE = RenderMode.RGB_ARRAY_LIST\n",
    "BEHAVIOR_POLICY =  BehaviorPolicyType.random\n",
    "\n",
    "\n",
    "#Create env\n",
    "register_grid_envs()\n",
    "env = gym.make(ENV_NAME, render_mode=RENDER_MODE)\n",
    "\n",
    "render_custom_policy_simple_grid(\n",
    "    env_name=ENV_NAME,\n",
    "    render_mode=RENDER_MODE,\n",
    "    behavior_policy=BEHAVIOR_POLICY,\n",
    "    num_steps=NUM_STEPS,\n",
    ")\n",
    "\n",
    "\n",
    "#Load data\n",
    "#data = load_buffer_minari(DATA_SET_NAME)\n",
    "#print(f\"number of elements: {len(data)}\")\n",
    "\n",
    "#state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(data, env, policy=None, num_episodes=NUM_EPISODES)\n",
    "#state_action_histogram(state_action_count_data)\n",
    "\n",
    "#state_action_histogram(state_action_count_policy)\n",
    "#compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why is offline RL a difficult problem?\n",
    "\n",
    "\n",
    "1 - **Not possible to improve exploration**: As the learning algorithm must rely entirely on the static\n",
    "dataset $D$, there is no possibility of improving exploration: if D does not contain transitions that illustrate high-reward regions for our task it may be impossible to discover those regions. If we explore beyond our data we could have severe problems as there could be a good reason why this data is not in our dataset: maybe there is an obstacle that could damage the robot or a fragile object that could be damaged by the robot!\n",
    "\n",
    "Note that this is opposite to online RL where you explore by interacting with the environment. \n",
    "\n",
    "This is why the collecting data phase is so important!!\n",
    "\n",
    "2 - **Distributional shift**: state-action pair distribution in $D$ does not accurately represent the distribution of states-actions of the trained policy. This challenges many existing machine learning methods, which assume that data is independent and identically distributed (i.i.d.). In standard supervised learning, we aim to train a model that performs well on data from the same distribution as the training data. In offline RL, our goal is to learn a policy that behaves differently (hopefully better) than what's seen in the dataset $D$. As a consequence (see later) the RL algorithms will tend to generate actions not included in $D$ and so generate **out of distribution actions data**. This could be dangerous as during inference these actions could bring the system to unexplored states (i.e. not included in $D$).\n",
    "\n",
    "ToDo: See later some example about it. It could be a 2D grid where the data was collected considering an obstacle but in inference we use the same grid without obstacles and we start from an obstacle zone??? )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pdfexport presentation.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "tianshou_dev",
   "language": "python",
   "name": "tianshou_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
