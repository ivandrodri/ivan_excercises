requirements.txt

set environment variables (see dev.env)

pip install gym-simplegrid
pip install moviepy


ToDo:

URGENT!!!! :  ERROR: in visualization policy--> in training reward~8.3 but not reaching target (rew~3.05)

URGENT!!!! : In simple grid the input is dicrete(64) but I should use a one_hot_encoding!!!


0 - Factorize online trainings as I did for offline: decouple policy (i.e. config file + create_policy) from training
1 - generate_d4rl_datset not working at the moment.
2 - in registration the path to the environment file should be done automatically:
        entry_point = "examples.offline_RL_workshop.custom_envs.custom_grid_env_MINARI:create_custom_env"
3 - Try different rewards (sparce, distance, etc.)
4 - Try different action jumpy (so far only one site)
5 - Try different behavior policies (use combinations of them)
6 - Create a factory to restore policies and envs
7 - When factory is done the many offline trainings must simplify in only one file

8 -   _  _  _  _  _  _  S _  _  _   Use a rnd policy for data collector. A good algorithm will try to start around S
                    P=0

9 - Plot 1D out of distribution data --> see best way to do it.

10 - MiniGrid (2-D) grid worlds nice plots and difficulties.
    1 - Collect custom data
    2 - different off-policy algos.

11 - Example working: Discrete BCQ

a - Data generated by custom policy where state before target only has prob.
to go back (1,0) . Then:
     1 - for tau=0.70 it does the following PATTERN 1-1-1-0-1-0-1... (non out of ditribution)
     2 - for tau=0.75 it does the following PATTERN 1-1-1-1-0-1-0-1 ...  (non out of ditribution)
     3 - for tau=0.80  it does the following PATTERN 1-1-1-1-1 ...  (out of ditribution)

12 - Visualize OOD data and measure distance between them.

13 - Aff timeout to simple-grod 2d env and custom reward

14 - Try to understand a sense of how many data do you need in general (i.e. how sample inefficient it is for instance
regarding PPO)

15 - bug in policy/imitation/base.py --> the action should be categorical (line 47)

16 - Make widget plot of out of data vs offline-policy data (superpose and in terms
of params to see out of distribution data appearing - And corresponding simulation)




Open Issues in Tianshou:

Issue 1:

Support for Loading Minari Datasets

Currently, the repository provides a convenient method, load_buffer_d4rl, to load D4RL datasets for
offline RL algorithms. However, with the ongoing migration from D4RL to Minari, it is becoming increasingly
important to support Minari datasets loading directly.

Possibility to load minari datasets: So far there is only the possibility to add D4RL datasets
(load_buffer_d4rl function in examples/offline/utils.py) but it would be great to have the possibility to deal
with minari datasets for offline RL algorithms as d4rl is getting migrated to minari that will become the standard soon.


Issue 2:

There is a bug in Collector when rendering returns an image as collector just returns:

see line and code:

but there should be some visualizations.


Issue 3:

Minari combination datasets bug (if I dont allow lists as initial/target states there is an error...)


ToDo: offline_training.py --> changed to training.py : Save config to log/policy_name
    (create a class for serialization/deserialization)

ToDo: 1D grid is not working with minari -- Some issues with name conventions....